"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[8024],{5188:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"Data Platform with AWS CDK","metadata":{"permalink":"/data-on-eks/blog/Data Platform with AWS CDK","editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/blog/2022-11-28-emr-eks-emr-studio.md","source":"@site/blog/2022-11-28-emr-eks-emr-studio.md","title":"Data Platform with AWS CDK","description":"Introduction","date":"2022-11-28T00:00:00.000Z","formattedDate":"November 28, 2022","tags":[{"label":"emr-on-eks","permalink":"/data-on-eks/blog/tags/emr-on-eks"},{"label":"Spark","permalink":"/data-on-eks/blog/tags/spark"},{"label":"emr-studio","permalink":"/data-on-eks/blog/tags/emr-studio"},{"label":"CDK","permalink":"/data-on-eks/blog/tags/cdk"}],"readingTime":4.283333333333333,"hasTruncateMarker":false,"authors":[{"name":"aws","title":"Amazon Web Services","url":"https://github.com/aws/aws-emr-containers-best-practices","imageURL":"https://github.com/aws.png","key":"aws"}],"frontMatter":{"slug":"Data Platform with AWS CDK","title":"Data Platform with AWS CDK","authors":["aws"],"tags":["emr-on-eks","Spark","emr-studio","CDK"]},"nextItem":{"title":"Welcome","permalink":"/data-on-eks/blog/welcome"}},"content":"## Introduction\\n\\nIn this blog we will show you how you can use AWS CDK and the [Analytics Reference Architecture](https://aws.amazon.com/blogs/opensource/adding-cdk-constructs-to-the-aws-analytics-reference-architecture/) (ARA) library to deploy an end to end data analytics platform. This platform will allow you to run Spark interactive Session in Jupyter notebook with EMR Studio supported by EMR on EKS and run Spark jobs with EMR on EKS. The architecture below shows the infrasturcture you will deploy using the CDK and ARA library.\\n\\n![emr-eks-studio-ara-architecture](./emr-eks-studio-cdk-ara.png)\\n\\n## [Analytics Reference Architecture](https://aws.amazon.com/blogs/opensource/adding-cdk-constructs-to-the-aws-analytics-reference-architecture/)\\n\\nAWS Analytics Reference Architecture (ARA) exposes set fo reusable core components in an AWS CDK library, currently available in Typescript and Python. This library contains AWS CDK constructs (L3) that can be used to quickly provision analytics solutions in demos, prototypes, proofs of concept, and end-to-end reference architectures. The API of ARA Library is defined [here](https://constructs.dev/packages/aws-analytics-reference-architecture/v/2.4.11?lang=typescript). \\n\\nIn our case the library help you deploy an infrastructure optimised for Apache Spark running on EKS leveraging EMR on EKS. The infrastructure will out of the box provide you with pod collocation to reduce network traffic, deploy nodegroup in a single AZ to reduce cross AZ traffic during shuffle, use dedicated instances for EMR on EKS, use optimized instances for memory intensive jobs, use spot and on-demand instances for non-critical job and for critical jobs.\\n\\n## Prerequisites\\n\\nEnsure that you have installed the following tools on your machine.\\n\\n1. [aws cli](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)\\n2. [kubectl](https://Kubernetes.io/docs/tasks/tools/)\\n3. [CDK](https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#getting_started_install)\\n\\n## Solution\\n\\nTo deploy the data platform we will use an example in the `Analytics Reference Architecture`. The example is in the directory `examples/emr-eks-app` that you will find in the repository you will clone below.\\n\\nClone the repository\\n\\n```bash\\ngit clone https://github.com/aws-samples/aws-analytics-reference-architecture.git\\n```\\n\\nThis solution will deploy the following:\\n\\n- EKS cluster and a set of Nodegroups:\\n\\n    - Managed Nodegroup called tooling for running system critical pods. e.g., Cluster Autoscaler, CoreDNS, EBS CSI Driver..\\n    - Three Managed Nodegroup called critical for critical jobs, each in one AZ, this nodegroup use on-demand instances\\n    - Three Managed Nodegroup called non-critical for non-critical jobs, each in one AZ, this nodegroup use spot instances\\n    - Three Managed Nodegroup called notebook-driver for non-critical jobs, each in one AZ, this nodegroup use on-demand instances to have a stable driver.\\n    - Three Managed Nodegroup called notebook-executor for non-critical jobs, each in one AZ, this nodegroup use spot instances instances for executors.\\n\\n- Enable EKS Cluster to be with with EMR on EKS service\\n- EMR Virtual Cluster called `batchjob`, used to submitted jobs\\n- EMR Virtual Cluster called `emrvcplatform`, used to submitted jobs\\n- EMR Studio called `platform`\\n- A `managed endpoint`, called `platform-myendpoint` , to use with Jupyter notebooks you will create in the EMR Studio\\n- [Execution role](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/iam-execution-role.html) to use when submiting jobs with EMR on EKS `start-job-run`\\n- Execution role to use with managed endpoint.   \\n- pod templates stored in an S3 bucket called \\"EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION\\"\\n\\n### Customize\\n\\nThe infrastructure described above is defined in `emr-eks-app/lib/emr-eks-app-stack.ts`. If you want to customize it you can change the values in it. For example, you can chose not to create the default nodegroup to use for `jobs`, in this case you can set the `defaultNodeGroups` parameter to `false` in the `EmrEksCluster`. You can also call the `addEmrEksNodegroup` method to define your own nodegroups with specific labels, instances or taints. The `addEmrEksNodegroup` method is defined [here](https://constructs.dev/packages/aws-analytics-reference-architecture/v/2.4.11/api/EmrEksCluster?lang=typescript#addEmrEksNodegroup).\\n\\nYou can also create your own execution role through the `createExecutionRole` [method](https://constructs.dev/packages/aws-analytics-reference-architecture/v/2.4.11/api/EmrEksCluster?lang=typescript#createExecutionRole) or create a managed endpoint to attach it to an EMR Studio you deployed outside of the ARA library.\\n\\nIn order to simplify this example we use IAM authentication with IAM user for `EMR Studio`. If you would like to use a user in the `AWS IAM Identity Center` you can change `studioAuthMode` in the `NotebookPlatform` construct. Below you will can see the code snipet that you need to change.\\n\\n```ts\\nconst notebookPlatform = new ara.NotebookPlatform(this, \'platform-notebook\', {\\n      emrEks: emrEks,\\n      eksNamespace: \'dataanalysis\',\\n      studioName: \'platform\',\\n      studioAuthMode: ara.StudioAuthMode.IAM,\\n      });\\n```\\n\\n### Deploy\\n\\nBefore you run the solution, you **MUST** change the `eksAdminRoleArn` of the `props` object of `EmrEksCluster` in `lib/emr-eks-app-stack.ts`. This role allows you to interact manage EKS cluster and should have be allowed at least the IAM action `eks:AccessKubernetesApi`. You need to also change the `identityName` in the `addUser` method of the `NotebookPlatform` construct. The identityName **MUST BE** a valid IAM username that you use. Below you will can see the code snipet that you need to change.\\n\\n```ts\\nnotebookPlatform.addUser([{\\n        identityName:\'\',\\n        notebookManagedEndpoints: [{\\n        emrOnEksVersion: \'emr-6.8.0-latest\',\\n        executionPolicy: emrEksPolicy,\\n        managedEndpointName: \'myendpoint\'\\n              }],\\n      }]);\\n```\\n\\nLast you shold also update the IAM policies passed to the `createExecutionRole`, if you want to process data that is in S3 buckets that you own.\\n\\nNavigate into one of the example directories and run `cdk synth --profile YOUR-AWS-PROFILE`\\n\\n```bash\\ncd examples/emr-eks-app\\nnpm install\\ncdk synth --profile YOUR-AWS-PROFILE\\n```\\n\\nOnce the synth is completed you can deploy the infrastructrue with the following command:\\n\\n```bash\\ncdk deploy\\n```\\n\\nAt the end of the deployment you will see output like follow:\\n\\n![ara-cdk-output](./cdk-deploy-result.png)\\n\\nIn the output you will find job sample configurations with the best practices for Spark on Kubernetes like `dynamicAllocation` and `pod collocation`. \\n\\n### Job submission\\n\\nIn this example we will use the `crittical-job` job configuration to submit a job using that will compute `pi` using that is part of Spark distribution. \\nTo submit a job we will use Below you use `start-job-run` command with AWS CLI.\\n\\nBefore you run the command below, make sure to change update the following parameters with the on created by your own deployment. \\n    - <CLUSTER-ID> \u2013 The EMR virtual cluster ID, which you get from the AWS CDK output\\n    - <SPARK-JOB-NAME> \u2013 The name of your Spark job\\n    - <ROLE-ARN> \u2013 The execution role you created, which you get from the AWS CDK output\\n    - <S3URI-CRITICAL-DRIVER> \u2013 The Amazon S3 URI of the driver pod template, which you get from the AWS CDK output\\n    - <S3URI-CRITICAL-EXECUTOR> \u2013 The Amazon S3 URI of the executor pod template, which you get from the AWS CDK output\\n    - <Log_Group_Name> \u2013 Your CloudWatch log group name\\n    - <Log_Stream_Prefix> \u2013 Your CloudWatch log stream prefix\\n\\n\\n\\n<details>\\n<summary>AWS CLI for start-job-run command</summary>\\n\\n```bash\\n\\naws emr-containers start-job-run \\\\\\n    --virtual-cluster-id CLUSTER-ID\\\\\\n    --name=SPARK-JOB-NAME\\\\\\n    --execution-role-arn ROLE-ARN \\\\\\n    --release-label emr-6.8.0-latest \\\\\\n    --job-driver \'{\\n        \\"sparkSubmitJobDriver\\":{\\n        \\"entryPoint\\": \\"local:///usr/lib/spark/examples/src/main/python/pi.py\\"\\n        }\\n    }\' \\\\\\n    --configuration-overrides \'{\\n        \\"applicationConfiguration\\": [\\n            {\\n                \\"classification\\": \\"spark-defaults\\",\\n                \\"properties\\": {\\n                    \\"spark.hadoop.hive.metastore.client.factory.class\\": \\"com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory\\",\\n                    \\"spark.sql.catalogImplementation\\": \\"hive\\",\\n                    \\"spark.dynamicAllocation.enabled\\":\\"true\\",\\n                    \\"spark.dynamicAllocation.minExecutors\\": \\"8\\",\\n                    \\"spark.dynamicAllocation.maxExecutors\\": \\"40\\",\\n                    \\"spark.kubernetes.allocation.batch.size\\": \\"8\\",\\n                    \\"spark.executor.cores\\": \\"8\\",\\n                    \\"spark.kubernetes.executor.request.cores\\": \\"7\\",\\n                    \\"spark.executor.memory\\": \\"28G\\",\\n                    \\"spark.driver.cores\\": \\"2\\",\\n                    \\"spark.kubernetes.driver.request.cores\\": \\"2\\",\\n                    \\"spark.driver.memory\\": \\"6G\\",\\n                    \\"spark.dynamicAllocation.executorAllocationRatio\\": \\"1\\",\\n                    \\"spark.dynamicAllocation.shuffleTracking.enabled\\": \\"true\\",\\n                    \\"spark.dynamicAllocation.shuffleTracking.timeout\\": \\"300s\\",\\n                    \\"spark.kubernetes.driver.podTemplateFile\\": \\"s3://EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION/EKS-CLUSTER-NAME/pod-template/critical-driver.yaml\\",\\n                    \\"spark.kubernetes.executor.podTemplateFile\\": \\"s3://EKS-CLUSTER-NAME-emr-eks-assets-ACCOUNT-ID-REGION/EKS-CLUSTER-NAME/pod-template/critical-executor.yaml\\"\\n                }\\n            }\\n        ],\\n        \\"monitoringConfiguration\\": {\\n            \\"cloudWatchMonitoringConfiguration\\": {\\n                \\"logGroupName\\": \\"Log_Group_Name\\",\\n                \\"logStreamNamePrefix\\": \\"Log_Stream_Prefix\\"\\n            }\\n        }\\n    }\'\\n```\\n</details>\\n\\nVerify the job execution\\n\\n```bash\\nkubectl get pods --namespace=batchjob -w\\n```\\n\\n### Interactive session\\n\\nTo use an interactive session, you should log in to the EMR Studio instance with the URL provided to you at the end of `cdk deploy`. \\nThis link will be in the form of `https://es-xxxxx/emrstudio-prod-REGION.amazonaws.com`.\\nOnce you click on the link you will be see a log in page where you **MUST** sign-in with the username provided to the `addUser` method. When you sign in you should follow these steps.\\n\\n1. Create workspace, this will start for a Jupyter notebook\\n2. Connect to the Jupter notebook\\n3. Attach to a Virtual cluster, this would be have the following name \\"emrvcplatform\\" and chose an endpoint called \\"platform-myendpoint\\"\\n4. Open a notebook and select the PySpark kernel\\n5. You are now ready to perform analyse your data with Spark running on EMR on EKS.\\n\\n## Cleanup\\n\\nTo clean up your environment, you call the command below. This will destroy the EKS cluster with Node groups and VPC\\n\\n```bash\\ncdk destroy\\n```\\n\\n:::caution\\n\\nTo avoid unwanted charges to your AWS account, delete all the AWS resources created during this deployment\\n:::"},{"id":"welcome","metadata":{"permalink":"/data-on-eks/blog/welcome","editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/blog/2022-09-19-welcome/index.md","source":"@site/blog/2022-09-19-welcome/index.md","title":"Welcome","description":"Data on EKS Blogs & Benchmarks","date":"2022-09-19T00:00:00.000Z","formattedDate":"September 19, 2022","tags":[{"label":"aws","permalink":"/data-on-eks/blog/tags/aws"}],"readingTime":0.25,"hasTruncateMarker":false,"authors":[{"name":"Vara Bonthu","title":"Maintainer of Data on EKS @ AWS","url":"https://github.com/vara-bonthu","imageURL":"https://github.com/vara-bonthu.png","key":"vara-bonthu"}],"frontMatter":{"slug":"welcome","title":"Welcome","authors":["vara-bonthu"],"tags":["aws"]},"prevItem":{"title":"Data Platform with AWS CDK","permalink":"/data-on-eks/blog/Data Platform with AWS CDK"},"nextItem":{"title":"EMR on EKS Best Practices","permalink":"/data-on-eks/blog/EMR on EKS Best Practices"}},"content":"## Data on EKS Blogs & Benchmarks\\n\\nIn this section you will find `Blogs` and `Benchmark reports` for the following topics.\\n\\nAWS Data Analytics and ML blogs are featured a short blogs.\\n\\n\ud83d\ude80 [EMR on EKS](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html)\\n\\n\ud83d\ude80 [Spark on EKS](https://spark.apache.org/docs/latest/running-on-kubernetes.html)\\n\\n\ud83d\ude80 Custom Kubernetes Schedulers (e.g., [Apache YuniKorn](https://yunikorn.apache.org/), [Volcano](https://volcano.sh/en/))\\n\\n\ud83d\ude80 Job Schedulers (e.g., [Apache Airflow](https://airflow.apache.org/), [Argo Workflows](https://argoproj.github.io/argo-workflows/))\\n\\n\ud83d\ude80 Distributed Databases (e.g., [Cassandra](https://cassandra.apache.org/_/blog/Cassandra-on-Kubernetes-A-Beginners-Guide.html), [CockroachDB](https://github.com/cockroachdb/cockroach-operator), [MongoDB](https://github.com/mongodb/mongodb-kubernetes-operator) etc.)\\n\\n\ud83d\ude80 Streaming Platforms (e.g., [Apache Kafka](https://github.com/apache/kafka), [Apache Flink](https://github.com/apache/flink), Apache Beam etc.)"},{"id":"EMR on EKS Best Practices","metadata":{"permalink":"/data-on-eks/blog/EMR on EKS Best Practices","editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/blog/2022-09-10-emr-eks-best-practices.mdx","source":"@site/blog/2022-09-10-emr-eks-best-practices.mdx","title":"EMR on EKS Best Practices","description":"EMR Containers Best Practices Guides","date":"2022-09-10T00:00:00.000Z","formattedDate":"September 10, 2022","tags":[{"label":"emr-on-eks","permalink":"/data-on-eks/blog/tags/emr-on-eks"},{"label":"Spark","permalink":"/data-on-eks/blog/tags/spark"},{"label":"Observability","permalink":"/data-on-eks/blog/tags/observability"},{"label":"Amazon Managed Prometheus","permalink":"/data-on-eks/blog/tags/amazon-managed-prometheus"},{"label":"Amazon Managed Grafana","permalink":"/data-on-eks/blog/tags/amazon-managed-grafana"}],"readingTime":0.42333333333333334,"hasTruncateMarker":false,"authors":[{"name":"aws","title":"Amazon Web Services","url":"https://github.com/aws/aws-emr-containers-best-practices","imageURL":"https://github.com/aws.png","key":"aws"}],"frontMatter":{"slug":"EMR on EKS Best Practices","title":"EMR on EKS Best Practices","authors":["aws"],"tags":["emr-on-eks","Spark","Observability","Amazon Managed Prometheus","Amazon Managed Grafana"]},"prevItem":{"title":"Welcome","permalink":"/data-on-eks/blog/welcome"},"nextItem":{"title":"Observability EMR on EKS","permalink":"/data-on-eks/blog/Observability EMR on EKS"}},"content":"## [EMR Containers Best Practices Guides](https://aws.github.io/aws-emr-containers-best-practices/)\\n\\nAmazon EMR on Amazon EKS enables you to submit Apache Spark jobs on demand on Amazon Elastic Kubernetes Service (EKS) without provisioning clusters. With EMR on EKS, you can consolidate analytical workloads with your other Kubernetes-based applications on the same Amazon EKS cluster to improve resource utilization and simplify infrastructure management.\\n\\nThis link provides the best practices and templates to get started with Amazon EMR on EKS. We publish this guide on GitHub so we could iterate the content quickly, provide timely and effective recommendations for variety of concerns, and easily incorporate suggestions from the broader community.\\n\\nCheckout the EMR on EKS Best practices GitHub docs [here](https://aws.github.io/aws-emr-containers-best-practices/)\\n\\n### Architecture\\nThe following diagram illustrates the solution architecture Amazon EMR on EKS.\\n\\n![emr-eks-architecture](./emr-eks-architecture.png)"},{"id":"Observability EMR on EKS","metadata":{"permalink":"/data-on-eks/blog/Observability EMR on EKS","editUrl":"https://github.com/awslabs/data-on-eks/blob/main/website/blog/2022-05-03-emr-eks-amp-amg-blog.mdx","source":"@site/blog/2022-05-03-emr-eks-amp-amg-blog.mdx","title":"Observability EMR on EKS","description":"Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafana","date":"2022-05-03T00:00:00.000Z","formattedDate":"May 3, 2022","tags":[{"label":"emr-on-eks","permalink":"/data-on-eks/blog/tags/emr-on-eks"},{"label":"Spark","permalink":"/data-on-eks/blog/tags/spark"},{"label":"Observability","permalink":"/data-on-eks/blog/tags/observability"},{"label":"Amazon Managed Prometheus","permalink":"/data-on-eks/blog/tags/amazon-managed-prometheus"},{"label":"Amazon Managed Grafana","permalink":"/data-on-eks/blog/tags/amazon-managed-grafana"}],"readingTime":0.38666666666666666,"hasTruncateMarker":false,"authors":[{"name":"Vara Bonthu","title":"Maintainer of Data on EKS @ AWS","url":"https://github.com/vara-bonthu","imageURL":"https://github.com/vara-bonthu.png","key":"vara-bonthu"}],"frontMatter":{"slug":"Observability EMR on EKS","title":"Observability EMR on EKS","authors":["vara-bonthu"],"tags":["emr-on-eks","Spark","Observability","Amazon Managed Prometheus","Amazon Managed Grafana"]},"prevItem":{"title":"EMR on EKS Best Practices","permalink":"/data-on-eks/blog/EMR on EKS Best Practices"}},"content":"## [Monitoring Amazon EMR on EKS with Amazon Managed Prometheus and Amazon Managed Grafana](https://aws.amazon.com/blogs/mt/monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana/)\\n\\nIn this post, we will learn to build end-to-end observability for EMR on EKS Spark workloads by leveraging Amazon Managed Service for Prometheus to collect and store the metrics generated by Spark Applications. We will then use Amazon Managed Grafana to build dashboards for monitoring use cases\\n\\nCheckout the full blog [here](https://aws.amazon.com/blogs/mt/monitoring-amazon-emr-on-eks-with-amazon-managed-prometheus-and-amazon-managed-grafana/)\\n\\n### Architecture\\nThe following diagram illustrates the solution architecture for scraping Spark Driver and Executors\u2019 metrics, as well as writing to Amazon Managed Service for Prometheus.\\n\\n![emr-eks-amp-amg](./emr-eks-amp-amg.png)\\n\\n### Grafana Dashboard for Spark\\nThe following Grafana dashboard displays the EMR on EKS Spark job metrics with Driver and Executor details.\\n\\n![emr-eks-amp-amg-output](./emr-eks-amp-amg-output.png)"}]}')}}]);