"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[1651],{3905:(e,r,t)=>{t.d(r,{Zo:()=>u,kt:()=>d});var a=t(7294);function n(e,r,t){return r in e?Object.defineProperty(e,r,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[r]=t,e}function o(e,r){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);r&&(a=a.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var r=1;r<arguments.length;r++){var t=null!=arguments[r]?arguments[r]:{};r%2?o(Object(t),!0).forEach((function(r){n(e,r,t[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(t,r))}))}return e}function i(e,r){if(null==e)return{};var t,a,n=function(e,r){if(null==e)return{};var t,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],r.indexOf(t)>=0||(n[t]=e[t]);return n}(e,r);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],r.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var p=a.createContext({}),l=function(e){var r=a.useContext(p),t=r;return e&&(t="function"==typeof e?e(r):s(s({},r),e)),t},u=function(e){var r=l(e.components);return a.createElement(p.Provider,{value:r},e.children)},c={inlineCode:"code",wrapper:function(e){var r=e.children;return a.createElement(a.Fragment,{},r)}},k=a.forwardRef((function(e,r){var t=e.components,n=e.mdxType,o=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),k=l(t),d=n,m=k["".concat(p,".").concat(d)]||k[d]||c[d]||o;return t?a.createElement(m,s(s({ref:r},u),{},{components:t})):a.createElement(m,s({ref:r},u))}));function d(e,r){var t=arguments,n=r&&r.mdxType;if("string"==typeof e||n){var o=t.length,s=new Array(o);s[0]=k;var i={};for(var p in r)hasOwnProperty.call(r,p)&&(i[p]=r[p]);i.originalType=e,i.mdxType="string"==typeof e?e:n,s[1]=i;for(var l=2;l<o;l++)s[l]=t[l];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}k.displayName="MDXCreateElement"},3630:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>p,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>l});var a=t(7462),n=(t(7294),t(3905));const o={sidebar_position:3,sidebar_label:"Observability Spark on EKS"},s="Observability Spark on EKS",i={unversionedId:"spark-on-eks/observability-spark-on-eks",id:"spark-on-eks/observability-spark-on-eks",title:"Observability Spark on EKS",description:"Introduction",source:"@site/docs/spark-on-eks/observability-spark-on-eks.md",sourceDirName:"spark-on-eks",slug:"/spark-on-eks/observability-spark-on-eks",permalink:"/data-on-eks/docs/spark-on-eks/observability-spark-on-eks",draft:!1,editUrl:"https://github.com/awslabs/data-on-eks/blob/main/website/docs/spark-on-eks/observability-spark-on-eks.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3,sidebar_label:"Observability Spark on EKS"},sidebar:"docs",previous:{title:"Spark Operator with YuniKorn",permalink:"/data-on-eks/docs/spark-on-eks/spark-operator-yunikorn"},next:{title:"AI/ML on EKS",permalink:"/data-on-eks/docs/category/aiml-on-eks"}},p={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Deploying the Solution",id:"deploying-the-solution",level:2},{value:"Set up data and py script",id:"set-up-data-and-py-script",level:2},{value:"Spark Web UI",id:"spark-web-ui",level:2},{value:"Spark History Server",id:"spark-history-server",level:2},{value:"Prometheus",id:"prometheus",level:2},{value:"Grafana",id:"grafana",level:2}],u={toc:l};function c(e){let{components:r,...o}=e;return(0,n.kt)("wrapper",(0,a.Z)({},u,o,{components:r,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"observability-spark-on-eks"},"Observability Spark on EKS"),(0,n.kt)("h2",{id:"introduction"},"Introduction"),(0,n.kt)("p",null,"In this post, we will learn the Observability for Spark on EKS. We will use Spark History Server to watch Spark Applications logs and check the Spark job progress via the Spark Web UI. Amazon Managed Service for Prometheus is used to collect and store the metrics generated by Spark Applications and Grafana is used to build dashboards for monitoring use cases."),(0,n.kt)("h2",{id:"deploying-the-solution"},"Deploying the Solution"),(0,n.kt)("p",null,"We will reuse the previous Spark on Operator example. Please follow ",(0,n.kt)("a",{parentName:"p",href:"https://awslabs.github.io/data-on-eks/docs/spark-on-eks/spark-operator-yunikorn#deploying-the-solution"},"this link")," to provision resources"),(0,n.kt)("h2",{id:"set-up-data-and-py-script"},"Set up data and py script"),(0,n.kt)("p",null,"let's navigate to one example folder under spark-k8s-operator and run the shell script to upload data and py script to the S3 bucket created by terraform above."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"cd data-on-eks/analytics/terraform/spark-k8s-operator/examples/nvme-ephemeral-storage\n\n# replace <S3_BUCKET> with your S3 bucket and <REGION> with your region, then run\n./taxi-trip-execute.sh\n")),(0,n.kt)("h2",{id:"spark-web-ui"},"Spark Web UI"),(0,n.kt)("p",null,"When you submit a Spark application, Spark context is created which ideally gives you ",(0,n.kt)("a",{parentName:"p",href:"https://sparkbyexamples.com/spark/spark-web-ui-understanding/"},"Spark Web UI")," to monitor the execution of the application. Monitoring includes the following."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Spark configurations used"),(0,n.kt)("li",{parentName:"ul"},"Spark Jobs, stages, and tasks details"),(0,n.kt)("li",{parentName:"ul"},"DAG execution"),(0,n.kt)("li",{parentName:"ul"},"Driver and Executor resource utilization"),(0,n.kt)("li",{parentName:"ul"},"Application logs and many more ",(0,n.kt)("br",null))),(0,n.kt)("p",null,"When your application is done with the processing, Spark context will be terminated so your Web UI as well. and if you wanted to see the monitoring for already finished application, we cannot do it."),(0,n.kt)("p",null,'To try Spark web UI, let\'s update <S3_BUCKET> with your bucket name and <JOB_NAME> with "nvme-taxi-trip" in nvme-ephemeral-storage.yaml'),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"  kubectl apply -f nvme-ephemeral-storage.yaml\n")),(0,n.kt)("p",null,"Then run port forward command to expose spark web service."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward -n=spark &lt;SPARK_DRIVER_NAME&gt; 4040:4040\n")),(0,n.kt)("p",null,"Then open browser and enter localhost:4040. You can view your spark application like below."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"img.png",src:t(4578).Z,width:"1888",height:"934"})),(0,n.kt)("h2",{id:"spark-history-server"},"Spark History Server"),(0,n.kt)("p",null,"As mentioned above, spark web UI will be terminated once the spark job is done. This is where Spark history Server comes into the picture, where it keeps the history (event logs) of all completed applications and its runtime information which allows you to review metrics and monitor the application later in time."),(0,n.kt)("p",null,"In this example, we installed Spark history Server to read logs from S3 bucket. In your spark application yaml file, make sure you have the following setting:"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'sparkConf:\n    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.InstanceProfileCredentialsProvider"\n    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"\n    "spark.eventLog.enabled": "true"\n    "spark.eventLog.dir": "s3a://<your bucket>/logs/"\n')),(0,n.kt)("p",null,"Run port forward command to expose spark-history-server service."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server\n")),(0,n.kt)("p",null,"Then open browser and enter localhost:18085. You can view your spark history server like below.\n",(0,n.kt)("img",{alt:"img.png",src:t(8083).Z,width:"1894",height:"500"})),(0,n.kt)("h2",{id:"prometheus"},"Prometheus"),(0,n.kt)("p",null,"Spark users must add the following config to spark application yaml file to extract the metrics from Spark Driver and Executors. In the example, they are added into nvme-ephemeral-storage.yaml already."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},'"spark.ui.prometheus.enabled": "true"\n"spark.executor.processTreeMetrics.enabled": "true"\n"spark.kubernetes.driver.annotation.prometheus.io/scrape": "true"\n"spark.kubernetes.driver.annotation.prometheus.io/path": "/metrics/executors/prometheus/"\n"spark.kubernetes.driver.annotation.prometheus.io/port": "4040"\n"spark.kubernetes.driver.service.annotation.prometheus.io/scrape": "true"\n"spark.kubernetes.driver.service.annotation.prometheus.io/path": "/metrics/driver/prometheus/"\n"spark.kubernetes.driver.service.annotation.prometheus.io/port": "4040"\n"spark.metrics.conf.*.sink.prometheusServlet.class": "org.apache.spark.metrics.sink.PrometheusServlet"\n"spark.metrics.conf.*.sink.prometheusServlet.path": "/metrics/driver/prometheus/"\n"spark.metrics.conf.master.sink.prometheusServlet.path": "/metrics/master/prometheus/"\n"spark.metrics.conf.applications.sink.prometheusServlet.path": "/metrics/applications/prometheus/"\n')),(0,n.kt)("p",null,"Run port forward command to expose prometheus service."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl  port-forward service/prometheus-server   8080:80 -n prometheus\n")),(0,n.kt)("p",null,"Then open browser and enter localhost:8080. You can view your prometheus server like below.\n",(0,n.kt)("img",{alt:"img.png",src:t(201).Z,width:"1911",height:"762"})),(0,n.kt)("h2",{id:"grafana"},"Grafana"),(0,n.kt)("p",null,"Grafana has been installed. Use the command below to access with port forward."),(0,n.kt)("h1",{id:"get-grafana-password"},"get grafana password"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"kubectl  port-forward service/grafana 8080:80 -n grafana  \n")),(0,n.kt)("p",null,"login username is admin and password can get from secrets manager. You can import dashboard with ID: 7890."),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"img.png",src:t(6591).Z,width:"1897",height:"796"})))}c.isMDXComponent=!0},201:(e,r,t)=>{t.d(r,{Z:()=>a});const a=t.p+"assets/images/prometheus-spark-6b44472f727a400ea97c3e78dae319a1.png"},6591:(e,r,t)=>{t.d(r,{Z:()=>a});const a=t.p+"assets/images/spark-grafana-dashboard-663f9623e038dc15be3cab6e7d6d14ef.png"},8083:(e,r,t)=>{t.d(r,{Z:()=>a});const a=t.p+"assets/images/spark-history-server-e3a6062c788ed738fa1a4b7ab7f64b26.png"},4578:(e,r,t)=>{t.d(r,{Z:()=>a});const a=t.p+"assets/images/spark-web-ui-734c4ac84186b9491042d11b78098303.png"}}]);